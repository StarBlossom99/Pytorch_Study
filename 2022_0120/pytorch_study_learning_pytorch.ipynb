{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_study_learning_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+L6ha54nLO/jA4baeTz5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StarBlossom99/Pytorch_Study/blob/main/2022_0120/pytorch_study_learning_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU 상에서 실행가능한 n 차원 Tensor\n",
        "신경망을 구성하고 학습하는 과정에서의 자동미분(Automatic differentiation)\n",
        "\n",
        "\n",
        "3차 다항식을 사용하여, y = sin x 에 근사하는 문제\n",
        "\n",
        "신경망은 4개의 매개변수를 가지며, 정답과 신경망이 예측한 결과 사이의 유클리드 거리를 최소화하여 임의의 값으로 근하하는 Gradient Descent를 사용\n",
        "\n"
      ],
      "metadata": {
        "id": "Ii5gF2WNw7XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy를 사용한 신경망 구성 예시"
      ],
      "metadata": {
        "id": "e885MUf2xr7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "x = np.linspace(-math.pi, math.pi, 2000)\n",
        "y = np.sin(x)\n"
      ],
      "metadata": {
        "id": "H7efZ699xvGy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.linspace(start, end, num) =>> 배열 생성 함수 start부터 end까지 범위를 num 만큼 나누어서 각각의 원소가 배열에 들어간다. 즉 -pi부터 pi까지를 2000개의 작은 범위로 나누어 각 숫자가 x의 배열로 들어간다고 생각하면 된다."
      ],
      "metadata": {
        "id": "GIeEGKwGyfDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()\n",
        "# a,b,c,d 는 가중치(weight) -> 무작위로 초기화\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "  #forward propagation level: 예측 값 y를 계산\n",
        "  # y = d x^3 + c x^2 + b x + a\n",
        "  y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "  #loss computation\n",
        "  loss = np.square(y_pred - y).sum() # (예측값 - 정답)을 제곱해서 더하기\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss)\n",
        "  #100번마다 출력\n",
        "\n",
        "  #loss 값에 따른 a,b,c,d의 gradient를 계산하고 Back Propagation\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_a = grad_y_pred.sum()\n",
        "  grad_b = (grad_y_pred * x).sum()\n",
        "  grad_c = (grad_y_pred * x ** 2).sum()\n",
        "  grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "  a -= learning_rate * grad_a\n",
        "  b -= learning_rate * grad_b\n",
        "  c -= learning_rate * grad_c\n",
        "  d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2fkIdkIy2tH",
        "outputId": "f361601b-4cc3-48f6-e008-66145f06cd23"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 4471.282150106664\n",
            "199 2963.7380130643346\n",
            "299 1965.671101218443\n",
            "399 1304.84781808157\n",
            "499 867.2758527475238\n",
            "599 577.5049963445662\n",
            "699 385.59228864904355\n",
            "799 258.4764981167908\n",
            "899 174.2700932839276\n",
            "999 118.48169290440242\n",
            "1099 81.51595623161727\n",
            "1199 57.0188252755767\n",
            "1299 40.78221223221722\n",
            "1399 30.01894282803036\n",
            "1499 22.88275980192968\n",
            "1599 18.15053403115266\n",
            "1699 15.011849832558926\n",
            "1799 12.929672368299512\n",
            "1899 11.548075361403374\n",
            "1999 10.631127965746678\n",
            "Result: y = 0.011798119539457728 + 0.8167820177184951 x + -0.0020353719421130024 x^2 + -0.08764659409832157 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy를 이용한 배열들은 Gpu를 통한 연산의 가속화가 불가능하다. 하지만 Tensor는 numpy와 비슷한 기능을 하지만 Gpu를 통한 연산의 가속화가 가능하다."
      ],
      "metadata": {
        "id": "YIckMd-L1lOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "#device = torch.device(\"cuda:0\") # GPU 실행코드\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "a = torch.rand((), device=device, dtype=dtype)\n",
        "b = torch.rand((), device=device, dtype=dtype)\n",
        "c = torch.rand((), device=device, dtype=dtype)\n",
        "d = torch.rand((), device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "  y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "  #loss computation\n",
        "  loss = (y_pred - y).pow(2).sum().item() # (예측값 - 정답)을 제곱해서 더하기\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss)\n",
        "  #100번마다 출력\n",
        "\n",
        "  #loss 값에 따른 a,b,c,d의 gradient를 계산하고 Back Propagation\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_a = grad_y_pred.sum()\n",
        "  grad_b = (grad_y_pred * x).sum()\n",
        "  grad_c = (grad_y_pred * x ** 2).sum()\n",
        "  grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "  a -= learning_rate * grad_a\n",
        "  b -= learning_rate * grad_b\n",
        "  c -= learning_rate * grad_c\n",
        "  d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n",
        "\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLBy0VEF17X1",
        "outputId": "aea2ed3c-4847-4d1c-8573-4d505aeb4fa8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 689.721923828125\n",
            "199 475.0415954589844\n",
            "299 328.41400146484375\n",
            "399 228.15121459960938\n",
            "499 159.5134735107422\n",
            "599 112.47156524658203\n",
            "699 80.19391632080078\n",
            "799 58.02143859863281\n",
            "899 42.773277282714844\n",
            "999 32.275360107421875\n",
            "1099 25.039905548095703\n",
            "1199 20.04759407043457\n",
            "1299 16.59934425354004\n",
            "1399 14.215069770812988\n",
            "1499 12.564834594726562\n",
            "1599 11.421483993530273\n",
            "1699 10.628561019897461\n",
            "1799 10.078139305114746\n",
            "1899 9.695708274841309\n",
            "1999 9.42973804473877\n",
            "Result: y = 0.023251254111528397 + 0.8456553220748901 x + -0.004011230077594519 x^2 + -0.09175356477499008 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import math\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n",
        "\n",
        "# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n",
        "# requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를\n",
        "# 계산할 필요가 없음을 나타냅니다.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# 가중치를 갖는 임의의 텐서를 생성합니다. 3차 다항식이므로 4개의 가중치가 필요합니다:\n",
        "# y = a + b x + c x^2 + d x^3\n",
        "# requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가\n",
        "# 있음을 나타냅니다.\n",
        "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # 순전파 단계: 텐서들 간의 연산을 사용하여 예측값 y를 계산합니다.\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # 텐서들간의 연산을 사용하여 손실(loss)을 계싼하고 출력합니다.\n",
        "    # 이 때 손실은 (1,) shape을 갖는 텐서입니다.\n",
        "    # loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있습니다.\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # autograd 를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를 갖는\n",
        "    # 모든 텐서들에 대한 손실의 변화도를 계산합니다.\n",
        "    # 이후 a.grad와 b.grad, c.grad, d.grad는 각각 a, b, c, d에 대한 손실의 변화도를\n",
        "    # 갖는 텐서가 됩니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # 경사하강법(gradient descent)를 사용하여 가중치를 직접 갱신합니다.\n",
        "    # torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True 지만\n",
        "    # autograd에서는 이를 추적하지 않을 것이기 때문입니다.\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "\n",
        "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRqL4TS28Tne",
        "outputId": "0d384f9a-5ce7-4225-a6e8-d9f7c567f7d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 1396.0648193359375\n",
            "199 969.4525756835938\n",
            "299 674.6918334960938\n",
            "399 470.8200988769531\n",
            "499 329.6677551269531\n",
            "599 231.84292602539062\n",
            "699 163.9803924560547\n",
            "799 116.85887908935547\n",
            "899 84.10945129394531\n",
            "999 61.328399658203125\n",
            "1099 45.46807098388672\n",
            "1199 34.41682052612305\n",
            "1299 26.71035385131836\n",
            "1399 21.33220863342285\n",
            "1499 17.576190948486328\n",
            "1599 14.951193809509277\n",
            "1699 13.115388870239258\n",
            "1799 11.830689430236816\n",
            "1899 10.931097984313965\n",
            "1999 10.300780296325684\n",
            "Result: y = 0.03828645870089531 + 0.8438709378242493 x + -0.006605049595236778 x^2 + -0.09149975329637527 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class LegendrePolynomial3(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    torch.autograd.Function을 상속받아 사용자 정의 autograd Function을 구현하고,\n",
        "    텐서 연산을 하는 순전파 단계와 역전파 단계를 구현해보겠습니다.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        순전파 단계에서는 입력을 갖는 텐서를 받아 출력을 갖는 텐서를 반환합니다.\n",
        "        ctx는 컨텍스트 객체(context object)로 역전파 연산을 위한 정보 저장에 사용합니다.\n",
        "        ctx.save_for_backward 메소드를 사용하여 역전파 단계에서 사용할 어떤 객체도\n",
        "        저장(cache)해 둘 수 있습니다.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        역전파 단계에서는 출력에 대한 손실(loss)의 변화도(gradient)를 갖는 텐서를 받고,\n",
        "        입력에 대한 손실의 변화도를 계산해야 합니다.\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n",
        "\n",
        "# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n",
        "# requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할\n",
        "# 필요가 없음을 나타냅니다.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# 가중치를 갖는 임의의 텐서를 생성합니다. 3차 다항식이므로 4개의 가중치가 필요합니다:\n",
        "# y = a + b * P3(c + d * x)\n",
        "# 이 가중치들이 수렴(convergence)하기 위해서는 정답으로부터 너무 멀리 떨어지지 않은 값으로\n",
        "# 초기화가 되어야 합니다.\n",
        "# requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가\n",
        "# 있음을 나타냅니다.\n",
        "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 5e-6\n",
        "for t in range(2000):\n",
        "    # 사용자 정의 Function을 적용하기 위해 Function.apply 메소드를 사용합니다.\n",
        "    # 여기에 'P3'라고 이름을 붙였습니다.\n",
        "    P3 = LegendrePolynomial3.apply\n",
        "\n",
        "    # 순전파 단계: 연산을 하여 예측값 y를 계산합니다;\n",
        "    # 사용자 정의 autograd 연산을 사용하여 P3를 계산합니다.\n",
        "    y_pred = a + b * P3(c + d * x)\n",
        "\n",
        "    # 손실을 계산하고 출력합니다.\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # autograd를 사용하여 역전파 단계를 계산합니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # 경사하강법(gradient descent)을 사용하여 가중치를 갱신합니다.\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "\n",
        "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "208QRNox-N_k",
        "outputId": "8a2bc4b1-8e4a-43e0-c284-181c8440262b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 209.95834350585938\n",
            "199 144.66018676757812\n",
            "299 100.70249938964844\n",
            "399 71.03519439697266\n",
            "499 50.97850799560547\n",
            "599 37.403133392333984\n",
            "699 28.206867218017578\n",
            "799 21.97318458557129\n",
            "899 17.7457275390625\n",
            "999 14.877889633178711\n",
            "1099 12.93176555633545\n",
            "1199 11.610918998718262\n",
            "1299 10.71425724029541\n",
            "1399 10.10548210144043\n",
            "1499 9.692106246948242\n",
            "1599 9.411375045776367\n",
            "1699 9.220745086669922\n",
            "1799 9.091285705566406\n",
            "1899 9.003360748291016\n",
            "1999 8.943639755249023\n",
            "Result: y = -5.394172664097141e-09 + -2.208526849746704 * P3(1.367587154632588e-09 + 0.2554861009120941 x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# 이 예제에서, 출력 y는 (x, x^2, x^3)의 선형 함수이므로, 선형 계층 신경망으로 간주할 수 있습니다.\n",
        "# (x, x^2, x^3)를 위한 텐서를 준비합니다.\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "# 위 코드에서, x.unsqueeze(-1)은 (2000, 1)의 shape을, p는 (3,)의 shape을 가지므로,\n",
        "# 이 경우 브로드캐스트(broadcast)가 적용되어 (2000, 3)의 shape을 갖는 텐서를 얻습니다.\n",
        "\n",
        "# nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의합니다.\n",
        "# nn.Sequential은 다른 Module을 포함하는 Module로, 포함되는 Module들을 순차적으로 적용하여 \n",
        "# 출력을 생성합니다. 각각의 Linear Module은 선형 함수(linear function)를 사용하여 입력으로부터\n",
        "# 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장합니다.\n",
        "# Flatten 계층은 선형 계층의 출력을 `y` 의 shape과 맞도록(match) 1D 텐서로 폅니다(flatten).\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "\n",
        "# 또한 nn 패키지에는 주로 사용되는 손실 함수(loss function)들에 대한 정의도 포함되어 있습니다;\n",
        "# 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용하겠습니다.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "\n",
        "    # 순전파 단계: x를 모델에 전달하여 예측값 y를 계산합니다. Module 객체는 __call__ 연산자를 \n",
        "    # 덮어써서(override) 함수처럼 호출할 수 있도록 합니다. 이렇게 함으로써 입력 데이터의 텐서를 Module에 전달하여\n",
        "    # 출력 데이터의 텐서를 생성합니다.\n",
        "    y_pred = model(xx)\n",
        "\n",
        "    # 손실을 계산하고 출력합니다. 예측한 y와 정답인 y를 갖는 텐서들을 전달하고,\n",
        "    # 손실 함수는 손실(loss)을 갖는 텐서를 반환합니다.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # 역전파 단계를 실행하기 전에 변화도(gradient)를 0으로 만듭니다.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를 계산합니다.\n",
        "    # 내부적으로 각 Module의 매개변수는 requires_grad=True일 때 텐서에 저장되므로,\n",
        "    # 아래 호출은 모델의 모든 학습 가능한 매개변수의 변화도를 계산하게 됩니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # 경사하강법을 사용하여 가중치를 갱신합니다.\n",
        "    # 각 매개변수는 텐서이므로, 이전에 했던 것처럼 변화도에 접근할 수 있습니다.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n",
        "\n",
        "# list의 첫번째 항목에 접근하는 것처럼 `model` 의 첫번째 계층(layer)에 접근할 수 있습니다.\n",
        "linear_layer = model[0]\n",
        "\n",
        "# 선형 계층에서, 매개변수는 `weights` 와 `bias` 로 저장됩니다.\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "revNRHov-q1w",
        "outputId": "2a3e18bd-ed24-4e46-95b7-74ec7570f249"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 757.30712890625\n",
            "199 503.866943359375\n",
            "299 336.2474670410156\n",
            "399 225.38597106933594\n",
            "499 152.06256103515625\n",
            "599 103.56605529785156\n",
            "699 71.48947143554688\n",
            "799 50.27305603027344\n",
            "899 36.23983383178711\n",
            "999 26.957420349121094\n",
            "1099 20.817317962646484\n",
            "1199 16.75576400756836\n",
            "1299 14.069037437438965\n",
            "1399 12.291732788085938\n",
            "1499 11.115975379943848\n",
            "1599 10.338127136230469\n",
            "1699 9.82353687286377\n",
            "1799 9.483077049255371\n",
            "1899 9.257807731628418\n",
            "1999 9.108759880065918\n",
            "Result: y = -0.0019353401148691773 + 0.8402368426322937 x + 0.00033387800795026124 x^2 + -0.0909828394651413 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# 입력 텐서 (x, x^2, x^3)를 준비합니다.\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "# nn 패키지를 사용하여 모델과 손실 함수를 정의합니다.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "# optim 패키지를 사용하여 모델의 가중치를 갱신할 optimizer를 정의합니다.\n",
        "# 여기서는 RMSprop을 사용하겠습니다; optim 패키지는 다른 다양한 최적화 알고리즘을 포함하고 있습니다.\n",
        "# RMSprop 생성자의 첫번째 인자는 어떤 텐서가 갱신되어야 하는지를 알려줍니다.\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "for t in range(2000):\n",
        "    # 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산합니다.\n",
        "    y_pred = model(xx)\n",
        "\n",
        "    # 손실을 계산하고 출력합니다.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # 역전파 단계 전에, optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인) 갱신할\n",
        "    # 변수들에 대한 모든 변화도(gradient)를 0으로 만듭니다. 이렇게 하는 이유는 기본적으로 \n",
        "    # .backward()를 호출할 때마다 변화도가 버퍼(buffer)에 (덮어쓰지 않고) 누적되기\n",
        "    # 때문입니다. 더 자세한 내용은 torch.autograd.backward에 대한 문서를 참조하세요.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 역전파 단계: 모델의 매개변수들에 대한 손실의 변화도를 계산합니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "linear_layer = model[0]\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsjrVl45-0Jc",
        "outputId": "6daa39db-d678-40f6-f79e-231434a04ae1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 6573.85107421875\n",
            "199 2254.858154296875\n",
            "299 1046.3116455078125\n",
            "399 653.8326416015625\n",
            "499 496.697265625\n",
            "599 387.9139404296875\n",
            "699 284.468505859375\n",
            "799 194.56011962890625\n",
            "899 124.71684265136719\n",
            "999 75.03299713134766\n",
            "1099 42.19434356689453\n",
            "1199 22.491079330444336\n",
            "1299 12.836446762084961\n",
            "1399 9.462677001953125\n",
            "1499 8.859138488769531\n",
            "1599 8.829158782958984\n",
            "1699 8.867586135864258\n",
            "1799 9.387723922729492\n",
            "1899 8.924385070800781\n",
            "1999 8.8922119140625\n",
            "Result: y = 5.473721031989953e-09 + 0.8571870923042297 x + -6.121982920603841e-09 x^2 + -0.09287617355585098 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class Polynomial3(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        생성자에서 4개의 매개변수를 생성(instantiate)하고, 멤버 변수로 지정합니다.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "        self.d = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        순전파 함수에서는 입력 데이터의 텐서를 받고 출력 데이터의 텐서를 반환해야 합니다.\n",
        "        텐서들 간의 임의의 연산뿐만 아니라, 생성자에서 정의한 Module을 사용할 수 있습니다.\n",
        "        \"\"\"\n",
        "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "\n",
        "    def string(self):\n",
        "        \"\"\"\n",
        "        Python의 다른 클래스(class)처럼, PyTorch 모듈을 사용해서 사용자 정의 메소드를 정의할 수 있습니다.\n",
        "        \"\"\"\n",
        "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
        "\n",
        "\n",
        "# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# 위에서 정의한 클래스로 모델을 생성합니다.\n",
        "model = Polynomial3()\n",
        "\n",
        "# 손실 함수와 optimizer를 생성합니다. SGD 생성자에 model.paramaters()를 호출해주면\n",
        "# 모델의 멤버 학습 가능한 (torch.nn.Parameter로 정의된) 매개변수들이 포함됩니다.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
        "for t in range(2000):\n",
        "    # 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산합니다.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # 손실을 계산하고 출력합니다.\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdo8L8NM-2WJ",
        "outputId": "55324118-d680-47ea-80da-8a7002e536b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 1292.9305419921875\n",
            "199 862.7638549804688\n",
            "299 576.9072265625\n",
            "399 386.885498046875\n",
            "499 260.5263366699219\n",
            "599 176.47116088867188\n",
            "699 120.53547668457031\n",
            "799 83.29753875732422\n",
            "899 58.496578216552734\n",
            "999 41.971458435058594\n",
            "1099 30.95538330078125\n",
            "1199 23.60820770263672\n",
            "1299 18.705402374267578\n",
            "1399 15.431900978088379\n",
            "1499 13.245089530944824\n",
            "1599 11.783271789550781\n",
            "1699 10.80550765991211\n",
            "1799 10.151054382324219\n",
            "1899 9.712727546691895\n",
            "1999 9.418927192687988\n",
            "Result: y = 0.012783578597009182 + 0.835985004901886 x + -0.0022053790744394064 x^2 + -0.0903780534863472 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class DynamicNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        생성자에서 5개의 매개변수를 생성(instantiate)하고 멤버 변수로 지정합니다.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "        self.d = torch.nn.Parameter(torch.randn(()))\n",
        "        self.e = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        모델의 순전파 단계에서는 무작위로 4, 5 중 하나를 선택한 뒤 매개변수 e를 재사용하여\n",
        "        이 차수들의의 기여도(contribution)를 계산합니다.\n",
        "\n",
        "        각 순전파 단계는 동적 연산 그래프를 구성하기 떄문에, 모델의 순전파 단계를 정의할 때\n",
        "        반복문이나 조건문과 같은 일반적인 Python 제어-흐름 연산자를 사용할 수 있습니다.\n",
        "\n",
        "        여기에서 연산 그래프를 정의할 때 동일한 매개변수를 여러번 사용하는 것이 완벽히 안전하다는\n",
        "        것을 알 수 있습니다.\n",
        "        \"\"\"\n",
        "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "        for exp in range(4, random.randint(4, 6)):\n",
        "            y = y + self.e * x ** exp\n",
        "        return y\n",
        "\n",
        "    def string(self):\n",
        "        \"\"\"\n",
        "        Python의 다른 클래스(class)처럼, PyTorch 모듈을 사용해서 사용자 정의 메소드를 정의할 수 있습니다.\n",
        "        \"\"\"\n",
        "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
        "\n",
        "\n",
        "# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# 위에서 정의한 클래스로 모델을 생성합니다.\n",
        "model = DynamicNet()\n",
        "\n",
        "# 손실 함수와 optimizer를 생성합니다. 이 이상한 모델을 순수한 확률적 경사하강법(SGD; Stochastic Gradient Descent)으로\n",
        "# 학습하는 것은 어려우므로, 모멘텀(momentum)을 사용합니다.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
        "for t in range(30000):\n",
        "    # 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산합니다.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # 손실을 계산하고 출력합니다.\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 2000 == 1999:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_CX7gxp-9Eo",
        "outputId": "15e6dd66-7d5c-4242-9803-604edae31821"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999 611.773681640625\n",
            "3999 293.4045715332031\n",
            "5999 141.07211303710938\n",
            "7999 74.03326416015625\n",
            "9999 39.16567611694336\n",
            "11999 22.990074157714844\n",
            "13999 15.357964515686035\n",
            "15999 11.881924629211426\n",
            "17999 10.244344711303711\n",
            "19999 9.505610466003418\n",
            "21999 9.16402530670166\n",
            "23999 8.967583656311035\n",
            "25999 8.687047958374023\n",
            "27999 8.874065399169922\n",
            "29999 8.879703521728516\n",
            "Result: y = -0.00317973829805851 + 0.8552684783935547 x + -2.1427857063827105e-05 x^2 + -0.09349662065505981 x^3 + 0.00012457817501854151 x^4 ? + 0.00012457817501854151 x^5 ?\n"
          ]
        }
      ]
    }
  ]
}