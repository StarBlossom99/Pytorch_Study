{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_study_autograd.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXgP1SypsEf0J9Vsr7GV9X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StarBlossom99/Pytorch_Study/blob/main/2022_0118/pytorch_study_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aYaLQygtz-fa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)\n",
        "y = torch.zeros(3)\n",
        "w = torch.randn(5,3)\n",
        "b = torch.randn(3)\n",
        "w.requires_grad_(True)\n",
        "b.requires_grad_(True)\n",
        "\n",
        "z = torch.matmul(x, w) + b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIvrhRPS1oMh",
        "outputId": "984cc6f4-a190-4889-b72c-4c644a9eb7b8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1736, -0.4479,  0.2425],\n",
            "        [-0.5950,  0.4032,  1.1620],\n",
            "        [-0.1828,  1.3298,  0.5295],\n",
            "        [ 3.1621, -0.7437, -0.6221],\n",
            "        [-1.2729,  1.4573, -0.6950]], requires_grad=True)\n",
            "tensor([ 0.1935, -2.2621, -0.2318], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "requires_grad의 값은 텐서를 생성할 때 설정하거나 나중에 x.requires_grad_(true) 메소드를 사용하여 나중에 설정할 수도 있다."
      ],
      "metadata": {
        "id": "Xa98_TP80g8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Gradient function for z = ', z.grad_fn)\n",
        "print('Gradient function for loss =', loss.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyXXhDZP02SG",
        "outputId": "d7b08edd-896c-4cc0-c88a-c3862f29c832"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z =  <AddBackward0 object at 0x7f0114bb4ad0>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f0114bb4f90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)\n",
        "\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ftPe211I3_",
        "outputId": "23162332-a684-4f0b-a0af-44703e9e858c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2520, 0.1448, 0.1984],\n",
            "        [0.2520, 0.1448, 0.1984],\n",
            "        [0.2520, 0.1448, 0.1984],\n",
            "        [0.2520, 0.1448, 0.1984],\n",
            "        [0.2520, 0.1448, 0.1984]])\n",
            "tensor([0.2520, 0.1448, 0.1984])\n",
            "tensor([[-0.1736, -0.4479,  0.2425],\n",
            "        [-0.5950,  0.4032,  1.1620],\n",
            "        [-0.1828,  1.3298,  0.5295],\n",
            "        [ 3.1621, -0.7437, -0.6221],\n",
            "        [-1.2729,  1.4573, -0.6950]], requires_grad=True)\n",
            "tensor([ 0.1935, -2.2621, -0.2318], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "requires_grad 속성의 값이 True로 설정된 노드들의 grad 속성만 구할 수 있고 다른 노드에는 변화도가 유효하지 않다. 성능상의 이유로 주어진 그래프에서의 backward를 사용한 변화도 계산은 한 번만 수행할 수 있고, 동일한 그래듶에서 여러번의 backward 호출이 필요하면, backward 호출시에 retrain_graph=True를 전달해야 한다."
      ],
      "metadata": {
        "id": "FByujDvz51tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)\n",
        "\n",
        "\n",
        "z = torch.matmul(x, w) + b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1JRVPBf4y7b",
        "outputId": "a4e022b0-74f7-4275-fac7-eff4e2051fc7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "requires_grad 속성 값이 True인 텐서의 변화도 추적을 멈추기 위해서는 torch.no_grad() 블록을 사용하거나 z.detach()를 이용하는 방법이 있다.\n",
        "\n",
        "변화도 추척을 멈춰야 하는 이유는 불필요한 텐서의 연산을 막아 연산 속도를 향상시키려는 목적과 일부 매개변수를 고정되게 사용하고 싶은 경우가 있을 수 있기 때문이다."
      ],
      "metadata": {
        "id": "VUidDM8P6MLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "autograd는 데이터에 실행된 모든 연산들의 기록을 Function 객체롤 구성된 방향성 비순환 그래프에 저장한다. 방향성 비순환 그래프(Directed Acyclic Graph)를 통해 뿌리(결과)에서부터 잎(입력)까지 추적하면 연쇄법칙에 따라 변화도를 자동으로 계산할 수 있다. \n",
        "\n",
        "forward propagation ->> 결과 텐서 계산, gradient function을 유지\n",
        "\n",
        "backward propagation ->> 각 텐서의 .grad_fn 으로부터 변화도를 계산하고, .grad 속성에 계산결과를 쌓고 연쇄법칙을 사용하여 모든 텐서들에게 propagate\n",
        "\n"
      ],
      "metadata": {
        "id": "dIddixWz8dxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.eye(5, requires_grad=True)\n",
        "print(inp)\n",
        "\n",
        "out = (inp+1).pow(2)\n",
        "print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iOIndh0-sTM",
        "outputId": "fde3d52f-b1e9-47fb-a750-a569c6e568bc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 1.]], requires_grad=True)\n",
            "tensor([[4., 1., 1., 1., 1.],\n",
            "        [1., 4., 1., 1., 1.],\n",
            "        [1., 1., 4., 1., 1.],\n",
            "        [1., 1., 1., 4., 1.],\n",
            "        [1., 1., 1., 1., 4.]], grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.eye(size) size * size 행렬 생성인데 대각선만 1이고 나머지는 0"
      ],
      "metadata": {
        "id": "u62dMfXe-_Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out.backward(torch.ones_like(inp), retain_graph=True)\n",
        "print(\"First call\\n\", inp.grad)\n",
        "out.backward(torch.ones_like(inp), retain_graph=True)\n",
        "print(\"\\nSecond call\\n\", inp.grad)\n",
        "inp.grad.zero_()\n",
        "out.backward(torch.ones_like(inp), retain_graph=True)\n",
        "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pUAsKLh-yQB",
        "outputId": "fdff9b94-1d07-411e-b05e-be2f55c50cf2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First call\n",
            " tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.],\n",
            "        [2., 2., 2., 2., 4.]])\n",
            "\n",
            "Second call\n",
            " tensor([[8., 4., 4., 4., 4.],\n",
            "        [4., 8., 4., 4., 4.],\n",
            "        [4., 4., 8., 4., 4.],\n",
            "        [4., 4., 4., 8., 4.],\n",
            "        [4., 4., 4., 4., 8.]])\n",
            "\n",
            "Call after zeroing gradients\n",
            " tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.],\n",
            "        [2., 2., 2., 2., 4.]])\n"
          ]
        }
      ]
    }
  ]
}